{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9919f3cb-9257-4d9c-b2be-b0bed558eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.5/186.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.26.2 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.35.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476afd24-1167-42da-8a54-73e126a4d696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00bdd817-741c-4c00-9a22-ca6a6d269100",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of batch normalization in the context of Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de6248-3e5d-4ff0-91cb-de91e0588a49",
   "metadata": {},
   "source": [
    "## Batch Normalization is a technique used in neural networks to stabilize and accelerate training. It normalizes the activations within mini-batches during training, reducing internal covariate shift. It helps networks converge faster, allows for higher learning rates, acts as regularization, and improves generalization. Batch Normalization is applied to layers and involves normalizing, scaling, and shifting activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ada0f1-4c1f-44d5-9043-506c8c15c356",
   "metadata": {},
   "source": [
    "# Q2. Describe the benefits of using batch normalization during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f646f6-5149-4b2f-b1e2-d50497e8af83",
   "metadata": {},
   "source": [
    "## 1. Faster convergence: Deep neural networks can be notoriously slow to train. Batch normalization helps address a problem called \"internal covariate shift,\" where the distribution of activations changes throughout training. This can make it difficult for gradients to flow back through the network and update the weights effectively. By normalizing the activations across each layer and batch, batch normalization reduces this shift and allows the network to converge faster, meaning it takes fewer training iterations to reach the desired performance.\n",
    "\n",
    "## 2. Higher learning rates: Typically, deep learning requires careful tuning of the learning rate, the parameter that controls how much the weights are updated in each iteration. A high learning rate can lead to instability and divergence, while a low learning rate can slow down training significantly. Batch normalization allows you to use larger learning rates without sacrificing stability, leading to faster training times.\n",
    "\n",
    "## 3. Improved generalization: Generalization refers to the network's ability to perform well on unseen data. Batch normalization can improve generalization by reducing the network's sensitivity to the initial weights and hyperparameters. This makes it less prone to overfitting to the training data and more likely to generalize well to new examples.\n",
    "\n",
    "## 4. Reduced dependence on weight initialization: Choosing good initial weights for a deep neural network can be crucial for successful training. Batch normalization helps alleviate this dependence by making the network less sensitive to the initial values. This can simplify the training process and make it more robust to different initialization schemes.\n",
    "\n",
    "## 5. Regularization effect: Batch normalization acts as a form of regularization, which helps to prevent overfitting. By reducing the variance of the activations, it makes it harder for the network to memorize the training data and encourages it to learn more generalizable features.\n",
    "\n",
    "## 6. Makes some activation functions viable: Certain activation functions, like tanh and sigmoid, can suffer from vanishing gradients in deep networks, which makes them difficult to train. Batch normalization can mitigate this issue by keeping the activations within a specific range, making these functions more effective in deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a1ac9-eab0-4036-a597-f3f54232b855",
   "metadata": {},
   "source": [
    "# Q3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d2156-2cbc-4f1c-bee4-0e4f56f2d0d7",
   "metadata": {},
   "source": [
    "## 1. Normalization Step: Calculate mean and variance: For each batch of activations (outputs of a layer) within a training iteration, batch normalization computes the mean (μ) and variance (σ²) across the batch.\n",
    "## Normalize activations: Each activation value within the batch is then normalized by subtracting the mean and dividing by the standard deviation (square root of variance), resulting in a distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "## 2. Learnable Parameters: Gamma (γ) and Beta (β): After normalization, two learnable parameters, γ and β, are introduced to restore flexibility and learn the optimal distribution for each layer.\n",
    "## Scaling and shifting: The normalized activations are scaled by γ and shifted by β, allowing the network to adjust the mean and variance of the activations if needed.\n",
    "\n",
    "##  3. Key Points: Layer-wise application: Batch normalization is typically applied to each layer's activations (excluding input and output layers).\n",
    "## Training vs. inference: During training, batch statistics (mean and variance) are calculated for each batch. During inference, the running averages of these statistics from training are used for normalization.\n",
    "## Regularization effect: Batch normalization acts as a regularizer, reducing overfitting by introducing noise to the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523941aa-e923-48fc-b601-347577f8c010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b70d57e-1c50-4cd7-8fca-7e0c62b0696c",
   "metadata": {},
   "source": [
    "# Q.2 IMPLEMENTATION\n",
    "## *Before Batch Normalization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66a298-4797-41f5-be7e-35fd3bdc695a",
   "metadata": {},
   "source": [
    "## 1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1833f470-07ea-43cc-a6fd-d61275a89497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4feb3a98-e7df-4936-ae1c-b825bb472a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Loading the FashionMnist Dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0 #Typecasting to float\n",
    "X_test = X_test / 255.0 #Typecasting to float\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1069f4eb-10d6-4b10-9271-34f479ebc062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape , y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391d70c5-9bf9-41eb-97b4-d1ebc0e03fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28), (55000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b5ed7a-e61a-4293-8dd3-ba02c1cab9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "559c7355-0194-4b9e-9b83-fc4507a62b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 28, 28), (5000, 28, 28))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape , X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b0df3f-69a6-4747-b453-a8b2e54cf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating layer of model\n",
    "\n",
    "#Setting seed for code reproducability\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "LAYERS = [ tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")]\n",
    "model = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b81d35-6510-4188-834d-923764d15db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bc31da6-ea88-4886-81ce-02d5095afe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 300)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bb3534f-6a6f-49c3-8b3a-ac7dbdc45ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 - 6s - loss: 1.5672 - accuracy: 0.5777 - val_loss: 0.9944 - val_accuracy: 0.7808 - 6s/epoch - 3ms/step\n",
      "Epoch 2/15\n",
      "1719/1719 - 5s - loss: 0.7839 - accuracy: 0.8147 - val_loss: 0.6180 - val_accuracy: 0.8490 - 5s/epoch - 3ms/step\n",
      "Epoch 3/15\n",
      "1719/1719 - 5s - loss: 0.5618 - accuracy: 0.8566 - val_loss: 0.4878 - val_accuracy: 0.8772 - 5s/epoch - 3ms/step\n",
      "Epoch 4/15\n",
      "1719/1719 - 5s - loss: 0.4713 - accuracy: 0.8748 - val_loss: 0.4235 - val_accuracy: 0.8896 - 5s/epoch - 3ms/step\n",
      "Epoch 5/15\n",
      "1719/1719 - 5s - loss: 0.4221 - accuracy: 0.8846 - val_loss: 0.3847 - val_accuracy: 0.8970 - 5s/epoch - 3ms/step\n",
      "Epoch 6/15\n",
      "1719/1719 - 5s - loss: 0.3906 - accuracy: 0.8923 - val_loss: 0.3590 - val_accuracy: 0.9020 - 5s/epoch - 3ms/step\n",
      "Epoch 7/15\n",
      "1719/1719 - 5s - loss: 0.3683 - accuracy: 0.8978 - val_loss: 0.3400 - val_accuracy: 0.9080 - 5s/epoch - 3ms/step\n",
      "Epoch 8/15\n",
      "1719/1719 - 5s - loss: 0.3515 - accuracy: 0.9017 - val_loss: 0.3249 - val_accuracy: 0.9138 - 5s/epoch - 3ms/step\n",
      "Epoch 9/15\n",
      "1719/1719 - 5s - loss: 0.3379 - accuracy: 0.9056 - val_loss: 0.3138 - val_accuracy: 0.9150 - 5s/epoch - 3ms/step\n",
      "Epoch 10/15\n",
      "1719/1719 - 5s - loss: 0.3268 - accuracy: 0.9084 - val_loss: 0.3036 - val_accuracy: 0.9178 - 5s/epoch - 3ms/step\n",
      "Epoch 11/15\n",
      "1719/1719 - 5s - loss: 0.3173 - accuracy: 0.9110 - val_loss: 0.2942 - val_accuracy: 0.9188 - 5s/epoch - 3ms/step\n",
      "Epoch 12/15\n",
      "1719/1719 - 5s - loss: 0.3089 - accuracy: 0.9131 - val_loss: 0.2868 - val_accuracy: 0.9216 - 5s/epoch - 3ms/step\n",
      "Epoch 13/15\n",
      "1719/1719 - 5s - loss: 0.3015 - accuracy: 0.9153 - val_loss: 0.2805 - val_accuracy: 0.9232 - 5s/epoch - 3ms/step\n",
      "Epoch 14/15\n",
      "1719/1719 - 5s - loss: 0.2948 - accuracy: 0.9171 - val_loss: 0.2752 - val_accuracy: 0.9242 - 5s/epoch - 3ms/step\n",
      "Epoch 15/15\n",
      "1719/1719 - 5s - loss: 0.2886 - accuracy: 0.9185 - val_loss: 0.2692 - val_accuracy: 0.9250 - 5s/epoch - 3ms/step\n",
      "Runtime of the program is 72.57323980331421\n"
     ]
    }
   ],
   "source": [
    "#Training and Calculating the training time\n",
    "import time\n",
    "#Starting time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_valid,y_valid),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "\n",
    "#Ending time\n",
    "end = time.time()\n",
    "\n",
    "#Total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed938-496c-42db-9873-3a796e5051c1",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "## val_accuracy: 0.9250\n",
    "## Runtime of the program is 72.57323980331421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b23f5-fc9e-4452-bd4d-7a9b4cebd1b7",
   "metadata": {},
   "source": [
    "# After Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b92b843-e1f0-4c4e-b016-5493135e571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the previous model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "064cbd82-6e68-48e3-8289-e2479ff03522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defing new model with batch normalization\n",
    "\n",
    "tf.random.set_seed(42)#Setting seed for code reproducability\n",
    "np.random.seed(42)\n",
    "\n",
    "LAYERS_BN = [\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "]\n",
    "\n",
    "model = tf.keras.models.Sequential(LAYERS_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f3c4a83-950f-492d-b061-1a5c1f727e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25386529-7dc2-4158-afd5-81756677f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15713e26-97a2-42c3-85e8-ef444d5d8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization/gamma:0 True\n",
      "batch_normalization/beta:0 True\n",
      "batch_normalization/moving_mean:0 False\n",
      "batch_normalization/moving_variance:0 False\n"
     ]
    }
   ],
   "source": [
    "for variable in bn1.variables:\n",
    "    print(variable.name, variable.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "542d82d2-ad39-411f-9ec5-339938523fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abfa5d1d-5464-49be-8bd7-435feaea338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 - 8s - loss: 0.8602 - accuracy: 0.7323 - val_loss: 0.4660 - val_accuracy: 0.8674 - 8s/epoch - 5ms/step\n",
      "Epoch 2/15\n",
      "1719/1719 - 6s - loss: 0.4392 - accuracy: 0.8721 - val_loss: 0.3434 - val_accuracy: 0.9016 - 6s/epoch - 4ms/step\n",
      "Epoch 3/15\n",
      "1719/1719 - 6s - loss: 0.3583 - accuracy: 0.8970 - val_loss: 0.2907 - val_accuracy: 0.9196 - 6s/epoch - 4ms/step\n",
      "Epoch 4/15\n",
      "1719/1719 - 6s - loss: 0.3122 - accuracy: 0.9093 - val_loss: 0.2612 - val_accuracy: 0.9260 - 6s/epoch - 4ms/step\n",
      "Epoch 5/15\n",
      "1719/1719 - 6s - loss: 0.2834 - accuracy: 0.9176 - val_loss: 0.2384 - val_accuracy: 0.9324 - 6s/epoch - 4ms/step\n",
      "Epoch 6/15\n",
      "1719/1719 - 6s - loss: 0.2629 - accuracy: 0.9231 - val_loss: 0.2215 - val_accuracy: 0.9380 - 6s/epoch - 4ms/step\n",
      "Epoch 7/15\n",
      "1719/1719 - 6s - loss: 0.2435 - accuracy: 0.9287 - val_loss: 0.2083 - val_accuracy: 0.9420 - 6s/epoch - 4ms/step\n",
      "Epoch 8/15\n",
      "1719/1719 - 6s - loss: 0.2323 - accuracy: 0.9321 - val_loss: 0.1955 - val_accuracy: 0.9450 - 6s/epoch - 4ms/step\n",
      "Epoch 9/15\n",
      "1719/1719 - 6s - loss: 0.2179 - accuracy: 0.9357 - val_loss: 0.1921 - val_accuracy: 0.9452 - 6s/epoch - 4ms/step\n",
      "Epoch 10/15\n",
      "1719/1719 - 7s - loss: 0.2091 - accuracy: 0.9394 - val_loss: 0.1823 - val_accuracy: 0.9480 - 7s/epoch - 4ms/step\n",
      "Epoch 11/15\n",
      "1719/1719 - 7s - loss: 0.1995 - accuracy: 0.9421 - val_loss: 0.1757 - val_accuracy: 0.9516 - 7s/epoch - 4ms/step\n",
      "Epoch 12/15\n",
      "1719/1719 - 6s - loss: 0.1916 - accuracy: 0.9449 - val_loss: 0.1699 - val_accuracy: 0.9510 - 6s/epoch - 4ms/step\n",
      "Epoch 13/15\n",
      "1719/1719 - 6s - loss: 0.1809 - accuracy: 0.9467 - val_loss: 0.1639 - val_accuracy: 0.9524 - 6s/epoch - 4ms/step\n",
      "Epoch 14/15\n",
      "1719/1719 - 6s - loss: 0.1760 - accuracy: 0.9491 - val_loss: 0.1599 - val_accuracy: 0.9540 - 6s/epoch - 4ms/step\n",
      "Epoch 15/15\n",
      "1719/1719 - 6s - loss: 0.1691 - accuracy: 0.9513 - val_loss: 0.1565 - val_accuracy: 0.9548 - 6s/epoch - 4ms/step\n",
      "Runtime of the program is 98.73910808563232\n"
     ]
    }
   ],
   "source": [
    "#Training and Calculating the training time\n",
    "\n",
    "#Starting time\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_valid,y_valid),\n",
    "                    verbose = 2\n",
    "                    )\n",
    "\n",
    "#Ending time\n",
    "end = time.time()\n",
    "\n",
    "#Total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ebdc1-db34-4e76-909b-42fd881cdb81",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "## val_accuracy: 0.9548\n",
    "## Runtime of the program is 98.73910808563232"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881f626-be45-4399-a905-55654c3afeb5",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "## Before Applying Batch Normalization\n",
    "## val_accuracy: 0.9250\n",
    "## Runtime of the program is 72.57323980331421\n",
    "\n",
    "# After Applying Batch Normalization\n",
    "## val_accuracy: 0.9548\n",
    "## Runtime of the program is 98.73910808563232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c30a2-b673-43e2-a08a-6b10fc975016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
